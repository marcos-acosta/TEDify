{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e054a99",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "This notebook walks through the entire process of loading, cleaning, and encoding TED Talks, followed by the training, testing, and saving of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c98fe",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5dc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEXT = 'NNNNN'\n",
    "APPLAUSE = 'AAAAA'\n",
    "LAUGHTER = 'LLLLL'\n",
    "reactions = [\"ðŸ˜\", \"ðŸ‘\", \"ðŸ˜‚\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CSV can be downloaded from https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset\n",
    "df = pd.read_csv('ted_talks_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7597afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = df[[(not x) for x in df['transcript'].isnull()]]['transcript']\n",
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a275be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize(text):\n",
    "    # Audience interactions\n",
    "    transcript = re.sub(r\"\\((.*?)\\)\", r\"\\n{\\1}.\\n\", text)\n",
    "    # Replace funky punctuation\n",
    "    transcript = re.sub(r\" \\.\\.\\.\", \",\", transcript)\n",
    "    transcript = re.sub(r\" â€”\", \",\", transcript)\n",
    "    transcript = re.sub(r\":\", \",\", transcript)\n",
    "    transcript = re.sub(r\";\", \",\", transcript)\n",
    "    transcript = re.sub(r\"!\", \".\", transcript)\n",
    "    sent_tokenize(transcript)\n",
    "    # Go through each line\n",
    "    lines = transcript.splitlines()\n",
    "    lines = [x.strip() for x in lines]\n",
    "    lines = [x for x in lines if x != '']\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        if line == '{Laughter}.':\n",
    "            sentences.append('LLLLL')\n",
    "        elif line in ['{Applause}.', '{Cheers}.', '{Cheers and applause}.', '{Applause and cheers}.', '{Applause, cheers}.', '{Audience cheers}.', '{Cheering}.']:\n",
    "            sentences.append('AAAAA')\n",
    "        elif len(line) >= 2 and line[0] != '{' and line[-2:] != '}.':\n",
    "            sentences += sent_tokenize(line)\n",
    "    return sentences\n",
    "\n",
    "def full_sentencize(texts):\n",
    "    sentences = []\n",
    "    for i in range(len(texts)):\n",
    "        sentences += ['NNNNN']\n",
    "        sentences += sentencize(texts[i])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365351ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and split sentences\n",
    "sentences = full_sentencize(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646da73",
   "metadata": {},
   "source": [
    "## Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e597ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider the top 20k words\n",
    "VOCAB_SIZE = 20000  \n",
    "# Only consider the first 200 words of each sample\n",
    "MAXLEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88217207",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_count = [\"``\", \"''\"]\n",
    "\n",
    "def get_encodings(text, vocab_size):\n",
    "    ''' Get word -> index and index -> word based on token frequency '''\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t not in dont_count]\n",
    "    word_freqs = dict(collections.Counter(tokens))\n",
    "    word_freqs_list = [(k, v) for k, v in word_freqs.items()]\n",
    "    sorted_frequencies = sorted(word_freqs_list, key=lambda x: -x[1])\n",
    "    abridged = sorted_frequencies[:vocab_size]\n",
    "    words = [x[0] for x in abridged]\n",
    "    word_to_index = {}\n",
    "    for i, word in enumerate(words):\n",
    "        word_to_index[word] = (i + 1)\n",
    "    return word_to_index\n",
    "\n",
    "def encode_sentence(text, word_to_index):\n",
    "    ''' Encode a sentence '''\n",
    "    tokenized = word_tokenize(text.lower())\n",
    "    return [(word_to_index[word] if (word in word_to_index) else 0) for word in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ebce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include the NEXT token as a token\n",
    "sentences_no_next = [s for s in sentences if s != NEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b425769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encodings\n",
    "word_to_index = get_encodings(' '.join(sentences_no_next), vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a81a4",
   "metadata": {},
   "source": [
    "Now we create our X and y data with a max-3-sentence sliding window to predict one of three reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "special = [APPLAUSE, LAUGHTER]\n",
    "\n",
    "def create_dataset(sentences, num_sentences=3):\n",
    "    ''' Create X and y data using sliding window approach '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(sentences)):\n",
    "        # Don't end a sentence with a special\n",
    "        if sentences[i] in (special + [NEXT]):\n",
    "            continue\n",
    "        # Encode sentence\n",
    "        current_batch = encode_sentence(sentences[i], word_to_index)\n",
    "        retrace_index = 0\n",
    "        sentence_count = 1\n",
    "        # Pre-append sentences to the list until we hit our desired no. context sentences\n",
    "        while sentence_count < num_sentences:\n",
    "            retrace_index += 1\n",
    "            index = i - retrace_index\n",
    "            # If we hit the beginning of the speech, give up\n",
    "            if index < 0 or sentences[index] == NEXT:\n",
    "                break\n",
    "            # Skip specials\n",
    "            elif sentences[index] in special:\n",
    "                continue\n",
    "            current_batch = encode_sentence(sentences[index], word_to_index) + current_batch\n",
    "            sentence_count += 1\n",
    "        X.append(current_batch)\n",
    "        # y is the reaction following this sentence\n",
    "        if i < len(sentences) - 1 and sentences[i + 1] in special:\n",
    "            y.append(special.index(sentences[i + 1]) + 1)\n",
    "        # If no reaction, reaction is 0\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return X, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of reactions\n",
    "sns.histplot(y, discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f16c4",
   "metadata": {},
   "source": [
    "This is highly imbalanced, so we'll synthetically undersample the \"no reaction\" X data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_zeros(X, y, rate=(1/8)):\n",
    "    ''' Downsample class 0 with given rate '''\n",
    "    X_keep = []\n",
    "    y_keep = []\n",
    "    for i in range(len(X)):\n",
    "        if not (y[i] == 0 and random.random() > rate):\n",
    "            X_keep.append(X[i])\n",
    "            y_keep.append(y[i])\n",
    "    return X_keep, np.array(y_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somewhat arbitrary\n",
    "X, y = downsample_zeros(X, y, rate=(3/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y, discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a3e36",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "First we define the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f98f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineTransformer(embed_dim=128, num_heads=2, ff_dim=64):\n",
    "    ''' Define and return Transformer with given parameters '''\n",
    "    inputs = layers.Input(shape=(MAXLEN,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(MAXLEN, VOCAB_SIZE + 1, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    ''' Tried two configurations: average pooling and, alternatively, last timestep only '''\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    # x = layers.Lambda(lambda x: x[:,-1])(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4f2a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = defineTransformer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33221dc0",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7223b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and testing data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a constant length\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAXLEN)\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574d822",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1faed67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "# Weights for minority class enforcing\n",
    "weights = {0: 1, 1: 1, 2: 1}\n",
    "\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=N_EPOCHS, \n",
    "    validation_data=(X_val, y_val), \n",
    "    class_weight=weights,\n",
    "    # Early stopping\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d170f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "    ''' Plot training and validation loss over epochs '''\n",
    "    loss_train = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "    epochs = range(0, len(loss_train))\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "    plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2737d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faaf0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_val)\n",
    "y_pred = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348081f1",
   "metadata": {},
   "source": [
    "### Qualitative test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84702e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_text = [\n",
    "    \"NNNNN\",\n",
    "    \"Hello everyone, and welcome to my TED Talk.\",\n",
    "    \"These past few weeks, I've been working on a project that I'm very excited to show you.\",\n",
    "    \"This program will listen to you as you speak, and determine whether what you just said was funny, impressive, or neither.\",\n",
    "    \"Then, it laughs or applauds accordingly, ensuring your genius never goes unnoticed.\",\n",
    "    \"With this application, people from around the world will finally be able to deliver a TED Talk from the comfort of their own home, and receive the attention they deserve.\",\n",
    "    \"Thank you very much.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows and pad\n",
    "toy_X, _ = create_dataset(toy_text)\n",
    "toy_X = keras.preprocessing.sequence.pad_sequences(toy_X, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "toy_pred = model.predict(toy_X)\n",
    "toy_pred = np.argmax(toy_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e4ffe",
   "metadata": {},
   "source": [
    "### Model persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(f'transformer_{SAVE_INDEX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fa661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encodings\n",
    "with open(f'word_to_index_{SAVE_INDEX}.json', 'w') as f:\n",
    "    json.dump(word_to_index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
